{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOx/GKtk+idMZVYoWx/B+M0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Najme-naseri/Image-Procssing/blob/main/ImageSqueeze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AN99M2tVO_nd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45ae7e6c-1c44-48f1-b9cf-9f526c1b961c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qF8FfxMj_MRR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import shutil\n",
        "from random import sample\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "import cv2\n",
        "import pdb\n",
        "import time\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from collections import OrderedDict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/MyDrive\""
      ],
      "metadata": {
        "id": "4By0tVEwPu3I"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "LdrNYq6-XXZ_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d alxmamaev/flowers-recognition --unzip"
      ],
      "metadata": {
        "id": "1KTTuiMPRMcN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64cad72e-f0e9-4f71-d061-29c2987cfdcb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading flowers-recognition.zip to /content\n",
            " 93% 209M/225M [00:01<00:00, 157MB/s]\n",
            "100% 225M/225M [00:01<00:00, 140MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "id": "PkSHYp7ef1Eo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fca1a8a5-ea23-44d3-cde1-258b6d2ff3bb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34mflowers\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# By defalt, set device to the CPU\n",
        "deviceFlag = torch.device('cpu')\n",
        "\n",
        "# Defalut is CPU, but as long as GPU is avaliable, then use GPU\n",
        "if torch.cuda.is_available():\n",
        " \n",
        "    print(f'Found {torch.cuda.device_count()} GPUs.')\n",
        "    deviceFlag = torch.device('cuda:0') # Manually pick your cuda device. By default is 'cuda:0'\n",
        "\n",
        "print(f'Now the deivce is set to {deviceFlag}')"
      ],
      "metadata": {
        "id": "DTZtrijBDHAB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d3f3045-3863-4ae0-c29a-f505f0621129"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now the deivce is set to cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DATA AUGMENTATION\n",
        "#transformation for train data\n",
        "\n",
        "training_transforms = transforms.Compose([\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(), # For GPU purpose\n",
        "    # As we are going to do transfer learning with a ImageNet pretrained squeezeNet\n",
        "    # so here we normalize the dataset being used here with the ImageNet stats\n",
        "    # for better transfer learning performance\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], # RGB mean & std estied on ImageNet\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "metadata": {
        "id": "4IvU8JcGixp5"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transformation for validation data\n",
        "validation_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], # RGB mean & std estied on ImageNet\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "f4zZ0GCUil2L"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transformation for test data\n",
        "testing_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], # RGB mean & std estied on ImageNet\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "Bs2AyES8mNvI"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Split Function\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "create Train, Test & Valid directory"
      ],
      "metadata": {
        "id": "qm0uwp32NFWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get data directory path and return train, test & valid path in a dictionary\n",
        "def split_data(flower_path):\n",
        "\n",
        "  #create path for train, test & valid\n",
        "    splited_data_path =  \"/splited_data\"\n",
        "    data_path = {'train': os.path.join(splited_data_path, \"train\"), \n",
        "            'valid': os.path.join(splited_data_path, \"valid\"),\n",
        "            'test':os.path.join(splited_data_path, \"test\")}\n",
        "\n",
        "  #specify the ratio of data\n",
        "    for phase in data_path.keys():\n",
        "      if phase == 'train':\n",
        "          ratio = 0.7\n",
        "      elif phase == 'test':\n",
        "          ratio = 0.2\n",
        "      else:\n",
        "          ratio = 0.1\n",
        "\n",
        "      #for each train, test & valid phase we create a directory for each flower\n",
        "      for name in os.listdir(flower_path):\n",
        "        \n",
        "        #create path for each flower\n",
        "        data_directory_path = os.path.join(data_path[phase], name)\n",
        "\n",
        "        #path of each flower data\n",
        "        flower_data_path = os.path.join(flower_path, name)\n",
        "\n",
        "        #make directory for each flower\n",
        "        os.makedirs(data_directory_path, exist_ok=True)\n",
        "\n",
        "        #Calculate the data size according to the phase\n",
        "        #this part have problem \n",
        "        data_size = math.floor(ratio * len(os.listdir(flower_data_path)))\n",
        "\n",
        "        #select random samples from data\n",
        "        selected_samples = sample(os.listdir(flower_data_path), data_size)\n",
        "\n",
        "        #for each sample, move data from flower directory\n",
        "        #to created directories for train, test & valid\n",
        "        #We use move so that we don't select duplicate data every time\n",
        "        for img in selected_samples:\n",
        "          shutil.move(os.path.join(flower_data_path, img), data_directory_path)\n",
        "\n",
        "\n",
        "    return data_path"
      ],
      "metadata": {
        "id": "URwCRlwBpKk2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "check split_data() function"
      ],
      "metadata": {
        "id": "ZRewiXD5s91X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the datasets with torchvision.datasets.ImageFolder object\n",
        "\n",
        "#use print to check that split_data() works?\n",
        "#call the function\n",
        "data_path = split_data(\"/content/flowers\")\n",
        "\n",
        "#for each flower directory\n",
        "for name in os.listdir(\"/content/flowers\"):\n",
        "  \n",
        "  #print size train\n",
        "  print(len(os.listdir(os.path.join(data_path['train'], name))))\n",
        "\n",
        "  #print size test\n",
        "  print(len(os.listdir(os.path.join(data_path['test'], name))))\n",
        "\n",
        "  #print size valid\n",
        "  print(len(os.listdir(os.path.join(data_path['valid'], name))))\n",
        "\n",
        "  #print the data that is not selected and remains in the flowers directory,\n",
        "  # the size of which should be 0\n",
        "  print(len(os.listdir(os.path.join(\"/content/flowers\", name))))\n",
        "\n",
        "#raining_imagefolder = datasets.ImageFolder(os.path.join(\"/content\", data_path['train']), transform = training_transforms)\n",
        "#validation_imagefolder = datasets.ImageFolder(os.path.join(\"/content\", data_path['valid']), transform = validation_transforms)\n",
        "#testing_imagefolder = datasets.ImageFolder(os.path.join(\"/content\", data_path['test']), transform = testing_transforms)"
      ],
      "metadata": {
        "id": "NIzw_5L5mwlV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e01c1af-71a1-42b0-ea89-efc76ced9a97"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "535\n",
            "41\n",
            "23\n",
            "165\n",
            "549\n",
            "42\n",
            "24\n",
            "169\n",
            "736\n",
            "57\n",
            "32\n",
            "227\n",
            "689\n",
            "53\n",
            "30\n",
            "212\n",
            "513\n",
            "40\n",
            "22\n",
            "158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tr_batchsize = 32\n",
        "eval_test_batchsize = 16\n",
        "epochs = 10\n",
        "lr = 0.001"
      ],
      "metadata": {
        "id": "NnMpDEoWoYSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the torch.utils.data.DataLoader() object with the ImageFolder object\n",
        "# Dataloader is a generator to read from ImageFolder and generate them into batch-by-batch\n",
        "# Only shuffle during trianing, validation and testing no shuffles\n",
        "# the batchsize for training and tesitng no need to be the same\n",
        "\n",
        "train_loader = DataLoader(training_imagefolder, batch_size = tr_batchsize, shuffle = True)\n",
        "validate_loader = DataLoader(validation_imagefolder, batch_size = eval_test_batchsize)\n",
        "test_loader = DataLoader(testing_imagefolder, batch_size = eval_test_batchsize)"
      ],
      "metadata": {
        "id": "0dobRhVDne0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Classes name to class index number Mapping with JSON\n",
        "import json\n",
        "with open('flower_to_name.json', 'r') as f:\n",
        "    flower_to_name = json.load(f)\n",
        "\n",
        "#print(len(flower_to_name))\n",
        "#print(flower_to_name)"
      ],
      "metadata": {
        "id": "TGjAciDYoGgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load pretrained squeezenet model\n",
        "model = models.squeezenet1_0(pretrained=True)"
      ],
      "metadata": {
        "id": "EZjfG4qMo2lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Freeze the params in the feature head, by setting .requries.grad to False\n",
        "for params in model.parameters():\n",
        "    params.requries_grad = False"
      ],
      "metadata": {
        "id": "iBlmSPzppO-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build a customed classifier head with 102 output classes\n",
        "#Change the last layer according to the problem\n",
        "\n",
        "NewClassifier = nn.Sequential(OrderedDict([\n",
        "    ('fc1', nn.Linear(25088, 4096)),\n",
        "    ('relu', nn.ReLU()),\n",
        "    ('drop', nn.Dropout(p = 0.5)),\n",
        "    ('fc2', nn.Linear(4096, 102)),\n",
        "    ('output', nn.LogSoftmax(dim = 1))\n",
        "]))"
      ],
      "metadata": {
        "id": "AUP2CUf2psDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Attach the new classifier head onto the model\n",
        "model.classifier = NewClassifier"
      ],
      "metadata": {
        "id": "c9IZFhwZp-iK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss func and the optimizer\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "# in the optimzier, define the range of params to be updated\n",
        "# Here we only train the params in the model.classifier part (model object has the classifier attribute)\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr = lr)"
      ],
      "metadata": {
        "id": "4b2sZA8tp0NY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The Validation Function\n",
        "\n",
        "def validation(model, validateloader, ValCriterion):\n",
        "    \n",
        "    val_loss_running = 0\n",
        "    acc = 0\n",
        "    \n",
        "    # a dataloader object is a generator of batches, each batch contain image & label separately\n",
        "    for images, labels in iter(validateloader):\n",
        "        \n",
        "        # Send the data onto choosen device\n",
        "        images = images.to(deviceFlag)\n",
        "        labels = labels.to(deviceFlag)\n",
        "        \n",
        "        output = model.forward(images)\n",
        "        val_loss_running += ValCriterion(output, labels).item() # .item() to get a scalar in Torch.tensor out\n",
        "        \n",
        "        output = torch.exp(output) # as in the model we use the .LogSoftmax() output layer\n",
        "        \n",
        "        equals = (labels.data == output.max(dim = 1)[1])\n",
        "        acc += equals.float().mean().item() # .flaot() is to transfer the tensor.cuda.float type onto cpu mode\n",
        "        \n",
        "    return val_loss_running / len(validateloader), acc / len(validateloader)"
      ],
      "metadata": {
        "id": "vcohEMLPn1se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The Training (&Validating) Function\n",
        "\n",
        "def train_eval(model, traindataloader, validateloader, TrCriterion, optimizer, epochs, deviceFlag_train):\n",
        "    \n",
        "    itrs = 0\n",
        "    eval_itrs = 40\n",
        "    \n",
        "    # first setting the device used for training\n",
        "    model.to(deviceFlag_train)\n",
        "    \n",
        "    print(f'The training batchsize is {tr_batchsize}.')\n",
        "    \n",
        "    # set the timer\n",
        "    since = time.time()\n",
        "\n",
        "    # ! THE EPOCH LOOP !\n",
        "    for e in range(epochs):        \n",
        "        itrs = 0\n",
        "        \n",
        "        # Set the model to the Train mode\n",
        "        # Tell the model to activate its Training behavior (turn-on the dropout & BN behaviors)\n",
        "        model.train()\n",
        "        \n",
        "        # re-initialize the running_loss to start every epoch\n",
        "        training_loss_running = 0\n",
        "        \n",
        "        #  ! THE BATCH LOOP !\n",
        "        for inputs, labels in iter(traindataloader):            \n",
        "            itrs += 1\n",
        "            # .to() method return a copy of the tensors on the targeted device\n",
        "            inputs = inputs.to(deviceFlag_train)\n",
        "            labels = labels.to(deviceFlag_train)\n",
        "            \n",
        "            # Clean the stored grads computed in the last iteration\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward Pass\n",
        "            # As model has been shipped to the targeted device, so the output is on that device too\n",
        "            outputs = model.forward(inputs)\n",
        "            \n",
        "            # Compute Loss\n",
        "            train_loss = TrCriterion(outputs, labels)\n",
        "            \n",
        "            # BackProp to compute the grads (stored in each tensor.grad() attributes) along the way\n",
        "            train_loss.backward()\n",
        "            \n",
        "            # Optimizer/Update params\n",
        "            optimizer.step()\n",
        "            \n",
        "            training_loss_running += train_loss.item() #numeric ops, take the scalar out of the tensor by calling .item()\n",
        "            \n",
        "            # ----------- Perform Validation (Evaluation) Every eval_itrs iterations ---------- #\n",
        "            if itrs % eval_itrs == 0:\n",
        "                \n",
        "                # Set the model to the Eval mode\n",
        "                model.eval()\n",
        "                \n",
        "                # Turn-off gradient for validation to save memory & computation\n",
        "                with torch.no_grad():\n",
        "                    validation_loss, val_acc = validation(model, validateloader, TrCriterion)\n",
        "                \n",
        "                display = f'Epoch: {e + 1}/{epochs}, itrs: {itrs}, '\n",
        "                display += f'Train_loss: {round(training_loss_running / eval_itrs, 4)}, '\n",
        "                display += f'Valid_loss: {round(validation_loss, 4)}, '\n",
        "                display += f'Valid_Acc: {round(val_acc, 4)}'\n",
        "                print(display)\n",
        "                \n",
        "                training_loss_running = 0\n",
        "                model.train()\n",
        "                \n",
        "        end = time.time()\n",
        "        elapsed = end - since\n",
        "        print(f'Epoch {e + 1} takees {round(elapsed, 4)} sec')"
      ],
      "metadata": {
        "id": "pSo6zelfL92A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The TEST function\n",
        "\n",
        "def test_acc(model, testloader, deviceFlag_test):\n",
        "\n",
        "    # for testing, it is actually do validation on the test set\n",
        "    model.eval()\n",
        "\n",
        "    model.to(deviceFlag_test)\n",
        "\n",
        "    since = time.time()\n",
        "\n",
        "    # In .eval() mode, set the context manager to turn-off grads\n",
        "    with torch.no_grad():\n",
        "        acc = 0\n",
        "\n",
        "        # iter() gives images and labels in batches\n",
        "        for inputs, labels in iter(test_loader):\n",
        "            \n",
        "            inputs = inputs.to(deviceFlag_test)\n",
        "            labels = labels.to(deviceFlag_test)\n",
        "\n",
        "            # Do a forward pass\n",
        "            output = model.forward(inputs)\n",
        "            # convert the log likelihood to scalar\n",
        "            prob = torch.exp(output)\n",
        "\n",
        "            equals = (labels.data == prob.max(dim = 1)[1])\n",
        "\n",
        "            acc += equals.type(torch.FloatTensor).mean().item()\n",
        "\n",
        "        end = time.time()\n",
        "        elapsed = end - since\n",
        "\n",
        "        print(f'Test_acc: {round(acc, 4)}, tiem_spent: {round(elapsed, 2)} sec')"
      ],
      "metadata": {
        "id": "r2V8D2o7MhJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_preprocessing(img_pth):\n",
        "    '''\n",
        "    Input a PIL image, output a numpy array with axes transposed to [Ch, H, W]\n",
        "    '''\n",
        "    pil_image = Image.open(img_pth)\n",
        "    \n",
        "    # -------- Resize with Aspect Ratio maintained--------- #\n",
        "    # First fixing the short axes\n",
        "    if pil_image.size[0] > pil_image.size[1]:\n",
        "        pil_image.thumbnail((10000000, 256))\n",
        "    else:\n",
        "        pil_image.thumbnail((256, 100000000))\n",
        "    \n",
        "    # ---------Crop----------- #\n",
        "    left_margin = (pil_image.width - 224) / 2\n",
        "    bottom_margin = (pil_image.height - 224) / 2\n",
        "    right_margin = left_margin + 224\n",
        "    top_margin = bottom_margin + 224\n",
        "    \n",
        "    pil_image = pil_image.crop((left_margin, bottom_margin, right_margin, top_margin))\n",
        "    \n",
        "    # --------- Convert to np then Normalize ----------- #\n",
        "    np_image = np.array(pil_image) / 255\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    np_image = (np_image -mean) / std\n",
        "    \n",
        "    # --------- Transpose to fit PyTorch Axes ----------#\n",
        "    np_image = np_image.transpose([2, 0, 1])\n",
        "    \n",
        "    return np_image"
      ],
      "metadata": {
        "id": "qCIyZPRyVuGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Image Dispalying Function with Matplotlib\n",
        "\n",
        "def imshow(pt_image, ax = None, title = None):\n",
        "    '''\n",
        "    Takes in a PyTorch-compatible image with [Ch, H, W],\n",
        "    Convert it back to [H, W, Ch], \n",
        "    Undo the preprocessing,\n",
        "    then display it on a grid\n",
        "    '''\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "    \n",
        "    # --------- Transpose ----------- #\n",
        "    plt_image = pt_image.transpose((1, 2, 0))\n",
        "    \n",
        "    # --------- Undo the preprocessing --------- #\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    plt_image = plt_image * std + mean\n",
        "    \n",
        "    if title is not None:\n",
        "        ax.set_title(title)\n",
        "        \n",
        "    # Image need to be clipped between 0 and 1 or it looks noisy\n",
        "    plt_image = np.clip(plt_image, 0, 1)\n",
        "    \n",
        "    # this imshow is a function defined in the plt module\n",
        "    ax.imshow(plt_image)\n",
        "    \n",
        "    return ax"
      ],
      "metadata": {
        "id": "qzTtzrwfV20q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prediction Function\n",
        "\n",
        "def predict(img_pth, model, trainingdataset, topk):\n",
        "    '''\n",
        "    1. input a single img;\n",
        "    2. forward pass on a model;\n",
        "    3. use tensor.topk(k) to return the highest k probs and the correspodniung class idx;\n",
        "    4. convert the idx to class names using the name_to_idx conversion.\n",
        "    '''\n",
        "    np_img = image_preprocessing(img_pth)\n",
        "    \n",
        "    # Convert np_img to PT tensor and send to GPU\n",
        "    pt_img = torch.from_numpy(np_img).type(torch.cuda.FloatTensor)\n",
        "    \n",
        "    # Unsqueeze to get shape of tensor from [Ch, H, W] to [Batch, Ch, H, W]\n",
        "    pt_img = pt_img.unsqueeze(0)\n",
        "\n",
        "    # Run the model to predict\n",
        "    output = model.forward(pt_img)\n",
        "    \n",
        "    probs = torch.exp(output)\n",
        "    \n",
        "    # Pick out the topk from all classes \n",
        "    top_probs, top_indices = probs.topk(topk)\n",
        "    \n",
        "    # Convert to list on CPU without grads\n",
        "    top_probs = top_probs.detach().type(torch.FloatTensor).numpy().tolist()[0]\n",
        "    top_indices = top_indices.detach().type(torch.FloatTensor).numpy().tolist()[0]\n",
        "    \n",
        "    # Invert the class_to_idx dict to a idx_to_class dict\n",
        "    idx_to_class = {value: key for key, value in trainingdataset.class_to_idx.items()}\n",
        "    top_classname = {idx_to_class[index] for index in top_indices}\n",
        "    \n",
        "    return top_probs, top_classname   "
      ],
      "metadata": {
        "id": "xzQgcSGlXDgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train & Eval the model\n",
        "train_eval(model, train_loader, validate_loader, criterion, optimizer, epochs, deviceFlag)"
      ],
      "metadata": {
        "id": "n2cSjDZAXZsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test the model\n",
        "test_acc(model, test_loader, deviceFlag)"
      ],
      "metadata": {
        "id": "GVYaGI0gXjme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = image_preprocessing('flowers/test/1/image_06743.jpg')\n",
        "imshow(image)"
      ],
      "metadata": {
        "id": "DfSqns9qX95M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs, classes = predict('flowers/test/15/image_06369.jpg', model, training_imagefolder, 5)   \n",
        "print(probs)\n",
        "print(classes)"
      ],
      "metadata": {
        "id": "nGRNkSP2YEKP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}